# 3DCV 2022 Final Project - Vision-Based Localization

## Get Started:
* Install the colmap: https://colmap.github.io/install.html
* Download models and place it at src/model: https://drive.google.com/drive/folders/1DJCcjF9xzN5Ls4qFzQhYElpztzg4NYKU?usp=sharing

## Dependencies:
* Python 3.8+
* Pytorch
* OpenCV
* Open3D

## Model Construction
`src/training/model_construction/` is for model construction.
<br>
### `SIFT_Colmap`
This directory is for constructing model using SIFT feature extraction.
1. Put image data in `images/`.
2. Run `reconstruct.sh build`.
If the construction result in 'no good initial images pair', try to add a number to lower the inlier constraint. ex:`reconstruct.sh build 100`.
3. Construction result will be place in `model/` with 5 files:
    * database.db
    * fused.ply
    * camera.bin
    * images.bin
    * points3D.bin

### `SuperPoint_Colmap`
This directory is for constructing model using SuperPoint feature extraction.
1. Put image data in `projdir/images/`.
2. Run `superC.sh`.
3. Construction result will be place in `model/` with 5 files:
    * database.db
    * fused.ply
    * camera.bin
    * images.bin
    * points3D.bin

### `SFM2DB`
This directory is for converting construction file to our own data format.
1. Put the 5 result files mentioned in the step 3 generated by one of above two construction method into `model/` directory under `SFM2DB`.
2. Put image data used for constructing model in to `images/`.
3. Run `CreateDB.py SIFT` for SIFT model or `CreateDB.py Superpoint` for SuperPoint model.
4. The output file will be named as `model.pkl` under `SFM2DB` directory.

## Model Compression
`src/training/model_compression/` is for model compression.
1. Run `run_compress.sh <model_path> <compress_model_name>`.
`<model_path>` is the path where the `model.pkl` generated from `SFM2DB`.
`<compress_model_name>` is the name of the output file.
2. Output files will be named as `<compress_model_name>_<K>.pkl`, `<compress_model_name>_pca_<K>.pkl`, `<compress_model_name>_local_<K>.pkl` with different `K` values : (20, 50, 100, 200).

## Camera Localization
`src/inference` is for Inference.
1. Run `run_exp.py` to reproduce our result.

For a single trial of experiment, please use `inference.py` as the following.
```
usage: inference.py [-h] [--input INPUT] [--camera_parameters CAMERA_PARAMETERS]
                    [--useSparse] [--sparse_model_file SPARSE_MODEL_FILE]
                    [--dense_model_file DENSE_MODEL_FILE]
                    [--feature_type FEATURE_TYPE] [--match_method MATCH_METHOD]
                    [--calculation_method CALCULATION_METHOD] [--use_ransac]
                    [--init_rvec INIT_RVEC INIT_RVEC INIT_RVEC]
                    [--init_tvec INIT_TVEC INIT_TVEC INIT_TVEC]
                    [--resize RESIZE RESIZE] [--view_scale VIEW_SCALE]

optional arguments:
  -h, --help            show this help message and exit
  --input INPUT         directory of sequential frames
  --camera_parameters CAMERA_PARAMETERS
                        npy file of camera parameters
  --useSparse           Use sparse or dense model for visualization
  --sparse_model_file SPARSE_MODEL_FILE
                        sparse 3D model file
  --dense_model_file DENSE_MODEL_FILE
                        Dense 3D model file
  --feature_type FEATURE_TYPE
                        Type of features extracted from image, e.g. SuperPoint,
                        SIFT, etc...
  --match_method MATCH_METHOD
                        Method to match feature points, e.g. Brute Force, KD-Tree
                        etc...
  --calculation_method CALCULATION_METHOD
                        Calculate method for solver
  --use_ransac          Whether to use ransac calculation to solve the pose
  --init_rvec INIT_RVEC INIT_RVEC INIT_RVEC
                        initial rotation vector
  --init_tvec INIT_TVEC INIT_TVEC INIT_TVEC
                        initial translation vector
  --resize RESIZE RESIZE
                        resize inference image to target size
  --view_scale VIEW_SCALE
                        scale of visuzalization
```
2. After running `inference.py`, the frames with information written on and the localization results will be saved inside `video/[video_name]`, where `video_name` refers to the inference setting. There will be a `log.txt` that reports the median execution time for each frame and the median FPS.

3. run `python video.py` to generate the video from frames, which will be stored at `video/`. The localization results will be combined into the frames. An extra file `SucRate.txt` will be generated, reporting the success rate of pose estimation for each test video.
 
### More Results
Inference videos can be found at: https://drive.google.com/drive/folders/1QuVm8hQINQjTym3AJXzGTbwrSLWh7nco?usp=sharing
The file names indicate the inference settings
